{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Verification and Extraction of Climate-Related Claims"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINER: Evidence Ranking (Step 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ER_Train_Arguments():\n",
    "    ev_path='./data/evidence.json'\n",
    "    train_claim_pickle = './pickles/er_train_claims.pkl'\n",
    "    er_model_name = 'roberta-large'\n",
    "    er_training_name = './models/evidence_ranking_training_folder'\n",
    "    er_save_dir = './models/evidence_ranking_trained_model'\n",
    "    dev_claims_pickle = './pickles/er_dev_claims.pkl'\n",
    "    learning_rate=1e-5\n",
    "    num_train_epochs=2\n",
    "    hard_neg_samples = 2\n",
    "    random_neg_samples = 2\n",
    "    neg_sample_range_low = 500\n",
    "    neg_sample_range_high = 1500\n",
    "    include_dev = True #Change to 'True' for final training run\n",
    "\n",
    "my_args = ER_Train_Arguments()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evidence dataframe from pickle\n",
    "def build_evidence(path):\n",
    "    print(\"Reading evidence from %s ...\" % path, end=\"\")\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(\"done.\")\n",
    "    evidence_list = []\n",
    "    for ev_id, text in data.items():\n",
    "        evidence_list.append([ev_id,text])\n",
    "    headers = [\"id\",\"text\"]\n",
    "    evidence = pd.DataFrame(evidence_list, columns=headers)\n",
    "    print(\"Number of ev: \", len(evidence))\n",
    "    return evidence\n",
    "\n",
    "# Get claims dataframe from pickle\n",
    "def get_claims_from_pickle(claim_pickle):\n",
    "    print(\"Getting claims from pickle %s.\" % claim_pickle)\n",
    "    with open(claim_pickle, 'rb') as f:\n",
    "        claims = pickle.load(f)\n",
    "    print(\"Number of claims: \", len(claims))\n",
    "    return claims\n",
    "\n",
    "# Build claim-evidence pairs for training:\n",
    "def build_pairs(claims, evidence, hard_neg_samples, random_neg_samples, neg_sample_range_low, neg_sample_range_high):\n",
    "    print(\"Building training pairs:\")\n",
    "    pairs = []\n",
    "    no_hard_neg_samples = hard_neg_samples\n",
    "    no_random_neg_samples = random_neg_samples\n",
    "\n",
    "    for _, row in tqdm(claims.iterrows(), total=len(claims)):\n",
    "        evidences = evidence[evidence['id'].isin(row['evidences'])]['text'].to_list()\n",
    "        for e in evidences:\n",
    "            pairs.append([row['text'],e,1])\n",
    "        hard_negative_samples = evidence[evidence['id'].isin(row['top_10k_consolidated'][neg_sample_range_low:neg_sample_range_high])].sample(len(evidences) * no_hard_neg_samples)\n",
    "        for _, hns in hard_negative_samples.iterrows():\n",
    "            pairs.append([row['text'],hns['text'],0])\n",
    "        random_negative_samples = evidence.sample(len(evidences) * no_random_neg_samples)\n",
    "        for _ , rns in random_negative_samples.iterrows():\n",
    "            pairs.append([row['text'], rns['text'],0])\n",
    "\n",
    "    headers = [\"claim_text\",\"ev_text\", \"labels\"]\n",
    "    pairs = pd.DataFrame(pairs, columns=headers)\n",
    "    print(\"Number of training pairs:\",len(pairs))\n",
    "    return pairs\n",
    "\n",
    "# Preprocessing function for mapping claim=ev pair text\n",
    "def preprocess_function(item):\n",
    "    \n",
    "    claim = item['claim_text']\n",
    "    evidence = item['ev_text']\n",
    "    encoded_input = tokenizer(\n",
    "        [[claim,evidence]],\n",
    "        add_special_tokens=True,\n",
    "        max_length=128, \n",
    "        truncation = True,\n",
    "        padding='max_length', \n",
    "        return_attention_mask=True, \n",
    "        return_tensors='pt' \n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoded_input['input_ids'].squeeze(),\n",
    "        'attention_mask': encoded_input['attention_mask'].squeeze(),\n",
    "        'labels': item['labels']\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting claims from pickle ./pickles/er_train_claims.pkl.\n",
      "Number of claims:  1228\n",
      "Getting claims from pickle ./pickles/er_dev_claims.pkl.\n",
      "Number of claims:  154\n",
      "Reading evidence from ./data/evidence.json ...done.\n",
      "Number of ev:  1208827\n",
      "Building training pairs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [01:56<00:00, 11.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training pairs: 23065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "claims = get_claims_from_pickle(my_args.train_claim_pickle)\n",
    "if my_args.include_dev:\n",
    "    claims = pd.concat([claims,get_claims_from_pickle(my_args.dev_claims_pickle)])\n",
    "evidence = build_evidence(my_args.ev_path)\n",
    "pairs = build_pairs(claims, evidence, my_args.hard_neg_samples, my_args.random_neg_samples, my_args.neg_sample_range_low, my_args.neg_sample_range_high)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f0290bd8d142539ab17f840ecc8331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "model_name = my_args.er_model_name\n",
    "training_name = my_args.er_training_name\n",
    "save_dir = my_args.er_save_dir\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "dataset = Dataset.from_pandas(pairs)\n",
    "encoded_dataset = dataset.map(preprocess_function)\n",
    "encoded_dataset.shuffle()\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    training_name,\n",
    "    learning_rate=my_args.learning_rate,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=my_args.num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    report_to=None,\n",
    "    save_strategy='no'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: ev_text, claim_text. If ev_text, claim_text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Program Files\\VirtualEnvs\\py_109_NLP\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 23065\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1775af6a934911bf40bae3501cc6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5768 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.369, 'learning_rate': 9.133148404993066e-06, 'epoch': 0.17}\n",
      "{'loss': 0.3153, 'learning_rate': 8.266296809986132e-06, 'epoch': 0.35}\n",
      "{'loss': 0.2919, 'learning_rate': 7.399445214979196e-06, 'epoch': 0.52}\n",
      "{'loss': 0.245, 'learning_rate': 6.5325936199722614e-06, 'epoch': 0.69}\n",
      "{'loss': 0.2216, 'learning_rate': 5.665742024965326e-06, 'epoch': 0.87}\n",
      "{'loss': 0.2221, 'learning_rate': 4.798890429958391e-06, 'epoch': 1.04}\n",
      "{'loss': 0.1539, 'learning_rate': 3.932038834951457e-06, 'epoch': 1.21}\n",
      "{'loss': 0.1383, 'learning_rate': 3.0651872399445217e-06, 'epoch': 1.39}\n",
      "{'loss': 0.1448, 'learning_rate': 2.198335644937587e-06, 'epoch': 1.56}\n",
      "{'loss': 0.1521, 'learning_rate': 1.331484049930652e-06, 'epoch': 1.73}\n",
      "{'loss': 0.111, 'learning_rate': 4.646324549237171e-07, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2727.2704, 'train_samples_per_second': 16.914, 'train_steps_per_second': 2.115, 'train_loss': 0.21179461644525832, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5768, training_loss=0.21179461644525832, metrics={'train_runtime': 2727.2704, 'train_samples_per_second': 16.914, 'train_steps_per_second': 2.115, 'train_loss': 0.21179461644525832, 'epoch': 2.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models/evidence_ranking_trained_model\n",
      "Configuration saved in ./models/evidence_ranking_trained_model\\config.json\n",
      "Model weights saved in ./models/evidence_ranking_trained_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_109_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
